{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a36cda9",
   "metadata": {},
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ba78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a7848",
   "metadata": {},
   "source": [
    "### Creating Session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499b094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:39:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FARS processing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrationRequired\", \"false\")\\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29b412",
   "metadata": {},
   "source": [
    "### Merging files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97ba1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(file_name):\n",
    "    \n",
    "    df1 = spark.read\\\n",
    "            .option('delimiter', ',')\\\n",
    "            .option('header', 'true')\\\n",
    "            .option('inferSchema', 'true')\\\n",
    "            .csv(f\"/FARS/Unclean_data/2020/{file_name}.CSV\")\n",
    "    \n",
    "    expected_schema = df1.schema.names\n",
    "    return expected_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ff0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files_for_year(year, file_name):\n",
    "\n",
    "    file_path = \"/FARS/Unclean_data/\" + str(year) + \"/\" + file_name + \".CSV\"\n",
    "    \n",
    "    # load the dataframe for the file\n",
    "    df = spark.read\\\n",
    "        .option('delimiter', ',')\\\n",
    "        .option('header', 'true')\\\n",
    "        .option('inferSchema', 'true')\\\n",
    "        .csv(file_path)\n",
    "\n",
    "    expected_schema = get_schema(file_name)\n",
    "    \n",
    "    # select the columns with string data type and convert them to integer\n",
    "    str_cols = [col_name for col_name, col_type in df.dtypes if col_type == 'string']\n",
    "    for col_name in str_cols:\n",
    "        df = df.withColumn(col_name, df[col_name].cast('integer'))\n",
    "    \n",
    "    missing_cols = set(expected_schema) - set(df.columns)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None).cast(IntegerType()))\n",
    "         \n",
    "    # select only the columns in the expected schema, and in the correct order\n",
    "    df = df.select(expected_schema)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf04f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating a list of dataframes for each year\n",
    "ACC = [merge_files_for_year(year, \"ACC_AUX\") for year in range(1982, 2021)]\n",
    "PER = [merge_files_for_year(year, 'PER_AUX') for year in range(1982, 2021)]\n",
    "VEH = [merge_files_for_year(year, 'VEH_AUX') for year in range(1982, 2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72297a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema(dfs):\n",
    "    for df2 in dfs:\n",
    "        if dfs[0].schema == df2.schema:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Schema for year not matched: \"+ str(df2.select('YEAR').first()[0]))\n",
    "    else:\n",
    "        print(\"Congrats!. All schemas are matched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08e50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats!. All schemas are matched.\n",
      "Congrats!. All schemas are matched.\n",
      "Congrats!. All schemas are matched.\n"
     ]
    }
   ],
   "source": [
    "check_schema(ACC)\n",
    "check_schema(PER)\n",
    "check_schema(VEH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dca8616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(dfs):\n",
    "    df = dfs[0].union(dfs[1])\n",
    "\n",
    "    for df2 in dfs[2:]:\n",
    "        df = df.union(df2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37642248",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_ACC = merge_dfs(ACC)\n",
    "m_PER = merge_dfs(PER)\n",
    "m_VEH = merge_dfs(VEH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acff9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(df):\n",
    "    num_rows = df.count()\n",
    "    num_cols = len(df.columns)\n",
    "    print(\"Number of rows: \", num_rows)\n",
    "    print(\"Number of columns: \", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef2120d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  1418963\n",
      "Number of columns:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 468:========================================>              (29 + 8) / 39]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shape(m_ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be7b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  3701950\n",
      "Number of columns:  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 471:===============================================>       (67 + 8) / 78]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shape(m_PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf0d419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  2133759\n",
      "Number of columns:  18\n"
     ]
    }
   ],
   "source": [
    "shape(m_VEH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ec430",
   "metadata": {},
   "source": [
    "### Selecting required columns and changing column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e848779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ST_CASE', 'YEAR', 'STATE', 'COUNTY', 'FATALS', 'A_CRAINJ', 'A_REGION', 'A_RU', 'A_INTER', 'A_RELRD', 'A_INTSEC', 'A_ROADFC', 'A_JUNC', 'A_MANCOL', 'A_TOD', 'A_DOW', 'A_CT', 'A_WEATHER', 'A_LT', 'A_MC', 'A_SPCRA', 'A_PED', 'A_PED_F', 'A_PEDAL', 'A_PEDAL_F', 'A_ROLL', 'A_POLPUR', 'A_POSBAC', 'A_D15_19', 'A_D16_19', 'A_D15_20', 'A_D16_20', 'A_D65PLS', 'A_D21_24', 'A_D16_24', 'A_RD', 'A_HR', 'A_DIST', 'A_DROWSY', 'BIA', 'SPJ_INDIAN', 'INDIAN_RES']\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(\"ACC_AUX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe766c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_COLS = [('ST_CASE', 'ST_CASE'), ('YEAR', 'YEAR'), ('STATE', 'State'), ('A_CRAINJ', 'Crash_injury_type'), \n",
    "            ('A_CT', 'Crash_type'), ('A_TOD', 'Time_of_day'), ('A_DIST', 'Distracted_driver'), \n",
    "            ('A_DOW', 'Day_of_week'), ('A_DROWSY', 'Drowsy_driver'), ('A_HR', 'Hit_run'), \n",
    "            ('A_INTSEC', 'Intersection'), ('A_JUNC', 'Junction'), ('A_LT', 'Involve_large_truck'), \n",
    "            ('A_MANCOL', 'Type_of_collision'), ('A_MC', 'Involve_motorcycle'), ('A_PED', 'Involve_pedestrain'), \n",
    "            ('A_POSBAC', 'Positive_blood_alcohol'), ('A_RD', 'Roadway_departure'), \n",
    "            ('A_ROADFC', 'Roadway_func_class'), ('A_ROLL', 'Crash_with_rollover'), ('A_SPCRA', 'Involve_speeding')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21c4ff8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YEAR', 'ST_CASE', 'VEH_NO', 'A_DRDIS', 'A_DRDRO', 'A_VRD', 'A_BODY', 'A_IMP1', 'A_IMP2', 'A_VROLL', 'A_LIC_S', 'A_LIC_C', 'A_CDL_S', 'A_MC_L_S', 'A_SPVEH', 'A_SBUS', 'A_MOD_YR', 'A_FIRE_EXP']\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(\"VEH_AUX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0a3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEH_COLS = [('ST_CASE', 'ST_CASE'), ('YEAR', 'YEAR'), ('A_BODY', 'Veh_body_type'), \n",
    "            ('A_DRDIS', 'Distracted_driver'), ('A_DRDRO', 'Drowsy_driver'), ('A_IMP1', 'Ini_impact_point'), \n",
    "            ('A_LIC_S', 'License_status'), ('A_SPVEH', 'Speeding_involved'), ('A_SBUS', 'School_bus'), \n",
    "            ('A_VROLL', 'Rollover')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7261fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_AGE1', 'A_AGE2', 'A_AGE3', 'A_AGE4', 'A_AGE5', 'A_AGE6', 'A_AGE7', 'A_AGE8', 'A_AGE9', 'ST_CASE', 'VEH_NO', 'PER_NO', 'YEAR', 'A_PTYPE', 'A_RESTUSE', 'A_HELMUSE', 'A_ALCTES', 'A_HISP', 'A_RCAT', 'A_HRACE', 'A_EJECT', 'A_PERINJ', 'A_LOC', 'A_DOA']\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(\"PER_AUX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c4dcc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "PER_COLS = [('ST_CASE', 'ST_CASE'), ('YEAR', 'YEAR'), ('A_AGE4', 'Age_group'), ('A_ALCTES', 'Alcohol_test'), \n",
    "        ('A_EJECT', 'Ejected'), ('A_HELMUSE', 'Helmet_use'), ('A_PERINJ', 'Person_injury_type'), \n",
    "        ('A_PTYPE', 'Person_type')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190fd0ac",
   "metadata": {},
   "source": [
    "### Changing and selecting required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f492b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = m_ACC.select([col(name[0]).alias(name[1]) for name in ACC_COLS])\n",
    "df_veh = m_VEH.select([col(name[0]).alias(name[1]) for name in VEH_COLS])\n",
    "df_per = m_PER.select([col(name[0]).alias(name[1]) for name in PER_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51c753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  1418963\n",
      "Number of columns:  21\n"
     ]
    }
   ],
   "source": [
    "shape(df_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71e0aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  2133759\n",
      "Number of columns:  10\n"
     ]
    }
   ],
   "source": [
    "shape(df_veh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98a55cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  3701950\n",
      "Number of columns:  8\n"
     ]
    }
   ],
   "source": [
    "shape(df_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273ffad",
   "metadata": {},
   "source": [
    "## Decoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5049f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "decode_dict1 = {}\n",
    "\n",
    "for name in df_acc.columns[3:]:\n",
    "    values1 = df_acc.select(name).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "    decode_dict1[name] = values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9a077e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Crash_injury_type': [1],\n",
       " 'Crash_type': [1, 3, 2],\n",
       " 'Time_of_day': [1, 3, 2],\n",
       " 'Distracted_driver': [1, 2],\n",
       " 'Day_of_week': [1, 3, 2],\n",
       " 'Drowsy_driver': [1, 2],\n",
       " 'Hit_run': [1, 2],\n",
       " 'Intersection': [1, 3, 2],\n",
       " 'Junction': [1, 3, 4, 2],\n",
       " 'Involve_large_truck': [1, 2],\n",
       " 'Type_of_collision': [1, 6, 3, 5, 4, 7, 2],\n",
       " 'Involve_motorcycle': [1, 2],\n",
       " 'Involve_pedestrain': [1, 2],\n",
       " 'Positive_blood_alcohol': [1, 3, 2],\n",
       " 'Roadway_departure': [3, 1, 2],\n",
       " 'Roadway_func_class': [1, 6, 3, 5, 4, 7, 2],\n",
       " 'Crash_with_rollover': [1, 2],\n",
       " 'Involve_speeding': [1, 2]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5248787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_dict1 = {\n",
    "#     \"STATE\": {1: 'Alabama', 2: 'Alaska', 4: 'Arizona', 5: 'Arkansas', 6: 'California', 8: 'Colorado', 9: 'Connecticut',\n",
    "#               10: 'Delaware', 11: 'District of Columbia', 12: 'Florida', 13: 'Georgia', 15: 'Hawaii', 16: 'Idaho',\n",
    "#               17: 'Illinois', 18: 'Indiana', 19: 'Iowa', 20: 'Kansas', 21: 'Kentucky', 22: 'Louisiana', 23: 'Maine',\n",
    "#               24: 'Maryland', 25: 'Massachusetts', 26: 'Michigan', 27: 'Minnesota', 28: 'Mississippi', 29: 'Missouri',\n",
    "#               30: 'Montana', 31: 'Nebraska', 32: 'Nevada', 33: 'New Hampshire', 34: 'New Jersey', 35: 'New Mexico',\n",
    "#               36: 'New York', 37: 'North Carolina', 38: 'North Dakota', 39: 'Ohio', 40: 'Oklahoma', 41: 'Oregon',\n",
    "#               42: 'Pennsylvania', 43: 'Puerto Rico', 44: 'Rhode Island', 45: 'South Carolina', 46: 'South Dakota',\n",
    "#               47: 'Tennessee', 48: 'Texas', 49: 'Utah', 50: 'Vermont', 51: 'Virginia', 52: 'Virgin Islands',\n",
    "#               53: 'Washington', 54: 'West Virginia', 55: 'Wisconsin', 56: 'Wyoming' },\n",
    "\n",
    "    \"Crash_injury_type\" : {1: \"Fatal\"},\n",
    "    \"Crash_type\": {1: \"Single_veh\", 2: \"Two_veh\", 3: \"More_than_2\"},\n",
    "    \"Time_of_day\": {1: \"Day\", 2: \"Night\", 3:\"Un\"},\n",
    "    \"Distracted_driver\": {1: \"Y\", 2: \"N\"},\n",
    "    \"Day_of_week\" : {1: \"W_day\", 2: \"W_end\", 3:\"Un\"},\n",
    "    \"Drowsy_driver\": {1: \"Y\", 2:\"N\"},\n",
    "    \"Hit_run\": {1: \"Y\", 2:\"N\"},\n",
    "    \"Intersection\": {1: \"Y\", 2:\"N\", 3:\"Un\"},\n",
    "    \"Junction\": {1: \"Junc\", 2: \"Non-junc\", 3:\"Oth\", 4:\"Un\"},\n",
    "    \"Involve_large_truck\": {1: \"Y\", 2:\"N\"},\n",
    "    \n",
    "#     \"Type_of_collision\": { 1: 'Non-Collision MVIT',\n",
    "#                            2: 'Rear-End', 3: 'Head-On', 4: 'Angle', 5: 'Sideswipe',\n",
    "#                            6: 'Oth', 7: 'Un'},\n",
    "    \"Involve_motorcycle\": {1: \"Y\", 2:\"N\"},\n",
    "    \"Involve_pedestrain\": {1: \"Y\", 2:\"N\"},\n",
    "    \"Positive_blood_alcohol\": {1:\"Y\", 2:\"N-D\", 3:\"Un\"},\n",
    "    \"Roadway_departure\": {1:\"Y\", 2:\"N\", 3:\"N-D\"},\n",
    "#     \"Roadway_func_class\" : {1: 'Interstate', 2: 'Fwy/Expwy', 3: 'Principal',\n",
    "#                             4: 'Minor', 5: 'Collector', 6: 'Local', 7: 'Un'},\n",
    "    \"Crash_with_rollover\": {1: \"Y\", 2:\"N\"},\n",
    "    \"Involve_speeding\": {1: \"Y\", 2:\"N\"},\n",
    "\n",
    "}\n",
    "\n",
    "for col_name in decode_dict1:\n",
    "    for key, value in decode_dict1[col_name].items():\n",
    "        df_acc = df_acc.withColumn(col_name, when(col(col_name) == key, value).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc5e10",
   "metadata": {},
   "source": [
    "#### We are not decoding few columns as it is significantly reducing the performance. So, we will use this labels directly as and when we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94e2e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "decode_dict2 = {}\n",
    "\n",
    "for name in df_veh.columns[2:]:\n",
    "    values2 = df_veh.select(name).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "    decode_dict2[name] = values2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cbbff76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Veh_body_type': [1, 6, 3, 5, 9, 4, 8, 7, 2],\n",
       " 'Distracted_driver': [1, 2],\n",
       " 'Drowsy_driver': [1, 2],\n",
       " 'Ini_impact_point': [1, 6, 3, 5, 4, 7, 2],\n",
       " 'License_status': [1, 3, 4, 2],\n",
       " 'Speeding_involved': [1, 2],\n",
       " 'School_bus': [1, 3, 2],\n",
       " 'Rollover': [1, 2]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d978b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary for column decoding\n",
    "decode_dict2 = {\n",
    "    \"Veh_body_type\" : {1: \"Pass_car\", 2: \"Pickup\", 3: \"Utility\", 4: \"Van\", 5: \"Li_Trk-oth\",\n",
    "                       6: \"Lrg_Truck\", 7: \"MC\", 8: \"Bus\", 9: \"Un\"},\n",
    "      \n",
    "    \"Distracted_driver\": {1: \"Y\", 2: \"N\"},\n",
    "    \"Drowsy_driver\": {1: \"Y\", 2: \"N\"},\n",
    "    \"Ini_impact_point\": {1: 'Non-Coll', 2: 'Front', 3: 'Right_S', 4: 'Rear', 5: 'Left_S', 6: 'Oth', 7: 'Un'},\n",
    "    \"License_status\": {1: \"Val\", 2: \"Inval\", 3: \"Un\", 4:\"NA\"},\n",
    "    \"Speeding_involved\": {1: \"Y\", 2: \"N\"},\n",
    "    \"School_bus\": {1: \"Y\", 2:\"Veh_as_bus\", 3:\"oth\"},\n",
    "    \"Rollover\": {1: \"Y\", 2: \"N\"} \n",
    "}\n",
    "\n",
    "for col_name in decode_dict2:\n",
    "    for key, value in decode_dict2[col_name].items():\n",
    "        df_veh = df_veh.withColumn(col_name, when(col(col_name) == key, value).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bac44a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "decode_dict3 = {}\n",
    "\n",
    "for name in df_per.columns[2:]:\n",
    "    values3 = df_per.select(name).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "    decode_dict3[name] = values3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a1d3001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age_group': [1, 6, 3, 5, 4, 8, 7, 2],\n",
       " 'Alcohol_test': [1, 3, 5, 4, 2],\n",
       " 'Ejected': [1, 3, 2],\n",
       " 'Helmet_use': [None, 1, 3, 2],\n",
       " 'Person_injury_type': [1, 6],\n",
       " 'Person_type': [1, 3, 5, 4, 2]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5005d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decode_dict3 = {\n",
    "    \"Age_group\" : {1: \"<16\", 2: \"16-20\", 3: \"21-24\", 4: \"25-34\", 5: \"35-44\", 6: \"45-64\", \n",
    "                   7: \"65+\", 8: \"Un\"},\n",
    "    \n",
    "    \"Alcohol_test\" : { 1: 'No-Alc', 2: '+ve_BAC', 3: 'Not_Tested', 4: 'Tested/Unknown',\n",
    "                       5: 'Un'},\n",
    "\n",
    "    \"Ejected\" : { 1: \"N\", 2: \"Y\", 3: \"Un\" },\n",
    "    \n",
    "    \"Helmet_use\": {1: \"Y\", 2: \"N\", 3: \"Un\"},\n",
    "    \n",
    "    \"Person_injury_type\": {1: 'Fatal', 2: 'Incapacitating Injured Estimate', 3: 'Nonincapacitating Injured Estimate', \n",
    "                           4: 'Other Injured Estimate', 5: 'Not Injured Estimate',\n",
    "                           6: 'Un', 7: 'NA'},\n",
    "    \n",
    "    \"Person_type\" : { 1: 'Driver', 2: 'Occupant', 3: 'Pedestrian', 4: 'cyclist',\n",
    "                    5: 'Un' }\n",
    "}\n",
    "\n",
    "for col_name in decode_dict3:\n",
    "    for key, value in decode_dict3[col_name].items():\n",
    "        df_per = df_per.withColumn(col_name, when(col(col_name) == key, value).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e140d",
   "metadata": {},
   "source": [
    "### Checking null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d07248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in df_acc.columns:\n",
    "    null_count = df_acc.where(df_acc[i].isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(col, null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f65d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 651:==================================================>    (36 + 3) / 39]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:43:18 WARN DAGScheduler: Broadcasting large task binary with size 14.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:43:52 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in df_veh.columns:\n",
    "    null_count = df_veh.where(df_veh[i].isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(i, null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d65dedbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:44:22 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:44:46 WARN DAGScheduler: Broadcasting large task binary with size 1004.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function col at 0x7fdb18265e10> 1760300\n",
      "23/03/17 19:45:01 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:45:17 WARN DAGScheduler: Broadcasting large task binary with size 1004.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in df_per.columns:\n",
    "    null_count = df_per.where(df_per[i].isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(col, null_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7355d26",
   "metadata": {},
   "source": [
    "#### Only helmet_use column has got missing values which can which is because this column was added later on in recent years there are still other columns with data not available but it will not impact in analysis purpose hence we do not need to do any treatment on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b55fb",
   "metadata": {},
   "source": [
    "## Saving data back to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d80fa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:45:40 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 705:>                                                       (0 + 8) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:45:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/17 19:45:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/17 19:45:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 705:===========>                                            (8 + 8) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:45:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/17 19:45:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/17 19:45:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 705:=================================>                     (24 + 8) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:45:51 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/17 19:45:51 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/17 19:45:51 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_acc.write.format(\"parquet\").option(\"compression\", \"snappy\").partitionBy(\"YEAR\").mode(\"overwrite\").save(\"/FARS/Clean_data/acc/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6c4c6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:46:03 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 706:>                                                       (0 + 8) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:46:04 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/17 19:46:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/17 19:46:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_veh.write.format(\"parquet\").option(\"compression\", \"snappy\").partitionBy(\"YEAR\").mode(\"overwrite\").save(\"/FARS/Clean_data/veh/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a6fc3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 19:46:22 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per.write.format(\"parquet\").option(\"compression\", \"snappy\").partitionBy(\"YEAR\").mode(\"overwrite\").save(\"/FARS/Clean_data/per/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33dfe07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
